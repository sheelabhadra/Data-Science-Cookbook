{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining and Harnessing Adversarial Examples\n",
    "## Authors: Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases, a wide variety of models with different architectures trained on different subsets of the training data misclassify the same adversarial example. This suggests that adversarial examples expose fundamental blind spots in our training algorithms.\n",
    "- Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples.\n",
    "\n",
    "### Insights\n",
    "- Let us add a small perturbation to the original input $x$ and call the new input $x^{*}$\n",
    "$$x^{*} = x + n$$\n",
    "$$x^{*} = x + \\epsilon W^{T}$$\n",
    "- If $w$ is large, $x^{*} \\neq x$. We can take $sign(w)$ so that we will always push $x^{*}$ in the positive direction.\n",
    "- As $x$ grows in dimension, the impact of $\\epsilon sign(W)$ on $\\hat{y}$ increases\n",
    "- **Fast Gradient Sign Method:** To generate adversarial examples $$x^{*} = x + \\epsilon sign(\\nabla_{x}J(w, x, y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
